# Green-Sense-CV-Project
Green Sense is a computer vision pipeline for automated urban vegetation quality monitoring from street-level municipal camera imagery. The project classifies green spaces into Healthy, Dried, and Contaminated conditions using synthetic data generation (FIBO), Vision Transformers (ViT-B16) and a hybrid CLIPâ€“DINOv2 architecture.

# ğŸŒ¿ Green Sense ğŸŒ¿  
### Urban Green Space Quality Monitoring from Street-Level Cameras

## Overview

Green Sense is an end-to-end computer vision pipeline designed to monitor the quality of urban green spaces using street-level municipal camera imagery.

The system classifies vegetation into three operational categories:
- ğŸŸ¢ **Healthy** â€“ Well-maintained, green vegetation with no visible litter  
- ğŸŸ¡ **Dried** â€“ Vegetation showing signs of water stress or decay  
- ğŸ”´ **Contaminated** â€“ Vegetation affected by visible litter or pollution  

The project addresses a key municipal challenge: scalable monitoring of green space health without costly manual inspections.

To overcome the lack of labeled street-camera data, Green Sense constructs a **synthetic dataset** using the FIBO generative AI model and evaluates multiple model families under domain shift conditions.
---

## ğŸš€ Key Contributions

- âœ… Creation of a **synthetic street-camera dataset** using controlled generative AI  
- âœ… Validation using **human screening + NIQE filtering**
- âœ… Domain shift evaluation (close-up â†’ street camera viewpoint)
- âœ… Comparison of three model families under identical protocols
- âœ… Hybrid architecture combining:
  - CLIP (vision-language)
  - DINOv2 (self-supervised vision)
  - Random Forest classifier
- âœ… Semantic auditing using Qwen2.5-VL for explainability
---

## ğŸ§  Models Evaluated

### 1ï¸âƒ£ ResNet50 Baseline
- ImageNet-pretrained
- Fine-tuned end-to-end
- Strong on close-up images
- Significant performance drop under street-camera domain shift

### 2ï¸âƒ£ Vision Transformer (ViT-B16)
- Global self-attention over image patches
- Improved robustness to scale and viewpoint changes
- Significantly outperforms ResNet50 in synthetic street-view setting

### 3ï¸âƒ£ CLIPâ€“DINOv2 Hybrid (Final Model)
- CLIP semantic embeddings
- DINOv2 visual embeddings
- Vegetation color & texture statistics
- Random Forest classifier
- Ensemble of semantic scores + classifier probabilities

This model achieves the strongest results.

---

## ğŸ“Š Results

### Synthetic Street Camera Test Set

| Model                  | Accuracy | F1   | Cohen Kappa | ROC AUC |
|------------------------|----------|------|-------------|---------|
| ResNet50               | 0.581    | 0.557| 0.375       | 0.796   |
| ViT-B16                | 0.905    | 0.902| 0.856       | 0.972   |
| **CLIPâ€“DINOv2 Hybrid** | **0.972**|**0.972**|**0.958** | **0.998** |

### Real-World Image Evaluation (33 images)

CLIPâ€“DINOv2 Hybrid:
- Accuracy: 0.849  
- ROC-AUC: 0.899  
- Demonstrates meaningful transfer without retraining

---

## ğŸ— Project Architecture
